{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5337151",
   "metadata": {},
   "source": [
    "# Modelagem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1c9e74",
   "metadata": {},
   "source": [
    "## Tratamento (condensado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3ca462",
   "metadata": {},
   "source": [
    "Aqui é feito o tratamento completo, que foi melhor explicado em \"treatment.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cf28a4",
   "metadata": {},
   "source": [
    "Mudanças: \n",
    "1. Exclui mais colunas (ver ``# Exclusoes mais novas``)\n",
    "2. Aglutina mais a coluna ``TP_ST_CONCLUSAO`` (ver ``# TP_CONCLUSAO``)\n",
    "3. Aglutina coluna ``ESC_PAI`` e ``ESC_MAE``\n",
    "4. Aglutina a coluna ``RENDA``\n",
    "5. Aglutina a coluna ``REGIAO``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cc8d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurações\n",
    "DATA_PATH = Path().resolve() / 'data'\n",
    "ARQUIVO_AMOSTRA_PATH = DATA_PATH / 'raw' / 'microdados_enem_2023_sample.csv'\n",
    "\n",
    "# Definição dos tipos\n",
    "colunas_float = [\n",
    "    'NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_LC', 'NU_NOTA_MT',\n",
    "    'NU_NOTA_COMP1', 'NU_NOTA_COMP2', 'NU_NOTA_COMP3', 'NU_NOTA_COMP4',\n",
    "    'NU_NOTA_COMP5', 'NU_NOTA_REDACAO'\n",
    "]\n",
    "\n",
    "colunas_string = [\n",
    "    'NU_INSCRICAO', 'CO_MUNICIPIO_ESC', 'CO_MUNICIPIO_PROVA'\n",
    "]\n",
    "\n",
    "# Captura os nomes das colunas\n",
    "colunas = pd.read_csv(ARQUIVO_AMOSTRA_PATH, nrows=0, encoding='latin1').columns.tolist()\n",
    "\n",
    "# Preparar o dicionário de tipos\n",
    "dtypes = {}\n",
    "for col in colunas:\n",
    "    if col in colunas_float:\n",
    "        dtypes[col] = 'float32'\n",
    "    elif col in colunas_string:\n",
    "        dtypes[col] = 'string'\n",
    "    else:\n",
    "        dtypes[col] = 'category'\n",
    "\n",
    "# Leitura com tipos otimizados\n",
    "df = pd.read_csv(ARQUIVO_AMOSTRA_PATH, dtype=dtypes, encoding='latin1')\n",
    "\n",
    "# REGIAO\n",
    "# Define regioes...\n",
    "df['REGIAO'] = df['CO_MUNICIPIO_PROVA'].apply(lambda x: x[0])\n",
    "df['REGIAO'] = df['REGIAO'].astype('category')\n",
    "\n",
    "#... e aglutina\n",
    "condicoes = [\n",
    "    (df['REGIAO'].isin(['1', '5'])),                 # N e C-O\n",
    "    (df['REGIAO'].isin(['3', '4'])),                 # S e SE\n",
    "    (df['REGIAO'].isin(['2'])),                     # NE\n",
    "]\n",
    "\n",
    "# Valores que serão atribuídos com base nas condições\n",
    "valores = [1, 2, 3]\n",
    "\n",
    "df['REGIAO'] =  np.select(condicoes, valores)\n",
    "df['REGIAO'] = df['REGIAO'].astype('category')\n",
    "\n",
    "df['REGIAO'] = df['REGIAO'].cat.rename_categories({\n",
    "    '1.0': '1',\n",
    "    '2.0': '2',\n",
    "    '3.0': '3'\n",
    "})\n",
    "\n",
    "# Exclusoes\n",
    "del df['NU_INSCRICAO']\n",
    "del df['TP_FAIXA_ETARIA']\n",
    "del df['CO_MUNICIPIO_ESC']                         \n",
    "del df['TP_DEPENDENCIA_ADM_ESC']                   \n",
    "del df['TP_LOCALIZACAO_ESC']                       \n",
    "del df['TP_SIT_FUNC_ESC']\n",
    "del df['TP_ENSINO']\n",
    "del df['CO_MUNICIPIO_PROVA']\n",
    "\n",
    "# TP_CONCLUSAO\n",
    "condicoes = [\n",
    "    (df['TP_ST_CONCLUSAO'] == '1'),                 # Concluido independente do\n",
    "    (df['TP_ST_CONCLUSAO'].isin(['2', '3'])),       # Cursando EM\n",
    "    (df['TP_ST_CONCLUSAO'] == '4')                  # EM não cursado\n",
    "]\n",
    "\n",
    "# Valores que serão atribuídos com base nas condições\n",
    "valores = [1, 2, 3]\n",
    "\n",
    "df['TP_CONCLUSAO'] =  np.select(condicoes, valores)\n",
    "df['TP_CONCLUSAO'] = df['TP_CONCLUSAO'].astype('category')\n",
    "\n",
    "df['TP_CONCLUSAO'] = df['TP_CONCLUSAO'].cat.rename_categories({\n",
    "    '1.0': '1',\n",
    "    '2.0': '2',\n",
    "    '3.0': '3'\n",
    "})\n",
    "\n",
    "del df['TP_ANO_CONCLUIU']\n",
    "del df['TP_ST_CONCLUSAO'] \n",
    "\n",
    "# TP_PRESENCA_DIA1 e TP_PRESENCA_DIA2\n",
    "df.rename(columns={\n",
    "    'TP_PRESENCA_CH': 'TP_PRESENCA_DIA1',\n",
    "    'TP_PRESENCA_CN': 'TP_PRESENCA_DIA2'\n",
    "}, inplace=True)\n",
    "\n",
    "del df['TP_PRESENCA_LC']\n",
    "del df['TP_PRESENCA_MT']\n",
    "\n",
    "# CO_PROVA_DIA1\n",
    "condicoes = [\n",
    "    df['CO_PROVA_CH'].isin(['1191.0', '1192.0', '1193.0', '1194.0']),               # Prova Comum\n",
    "    df['CO_PROVA_CH'].isin(['1195.0', '1196.0', '1197.0', '1198.0', '1199.0']),     # Prova Adaptada\n",
    "    df['CO_PROVA_CH'].isin(['1271.0', '1272.0', '1273.0', '1274.0']),               # Prova Reaplicação\n",
    "    df['CO_PROVA_CH'].isna()                                                        # Ausente\n",
    "]\n",
    "\n",
    "# Valores que serão atribuídos com base nas condições\n",
    "valores = [1, 2, 3, 4]\n",
    "\n",
    "df['CO_PROVA_DIA1'] =  np.select(condicoes, valores)\n",
    "df['CO_PROVA_DIA1'] = df['CO_PROVA_DIA1'].astype('category')\n",
    "del df['CO_PROVA_CH']\n",
    "del df['CO_PROVA_LC'] \n",
    "\n",
    "# CO_PROVA_DIA2\n",
    "condicoes = [\n",
    "    df['CO_PROVA_CN'].isin(['1221.0', '1222.0', '1223.0', '1224.0']),               # Prova Comum\n",
    "    df['CO_PROVA_CN'].isin(['1225.0', '1226.0', '1227.0', '1228.0', '1229.0']),     # Prova Adaptada\n",
    "    df['CO_PROVA_CN'].isin(['1301.0', '1302.0', '1303.0', '1304.0']),               # Prova Reaplicação\n",
    "    df['CO_PROVA_CN'].isna()                                            # Ausente\n",
    "]\n",
    "\n",
    "# Valores que serão atribuídos com base nas condições\n",
    "valores = [1, 2, 3, 4]\n",
    "\n",
    "df['CO_PROVA_DIA2'] = np.select(condicoes, valores)\n",
    "df['CO_PROVA_DIA2'] = df['CO_PROVA_DIA2'].astype('category')\n",
    "del df['CO_PROVA_CN']\n",
    "del df['CO_PROVA_MT'] \n",
    "\n",
    "# ESC_MAE, ESC_PAI, INTERNET\n",
    "df.rename(columns={\n",
    "    'Q001': 'ESC_PAI',\n",
    "    'Q002': 'ESC_MAE',\n",
    "    'Q025': 'INTERNET'\n",
    "}, inplace=True)\n",
    "\n",
    "# ESC_PAI\n",
    "condicoes = [\n",
    "    df['ESC_PAI'].isin(['A']),                         # Nunca estudou\n",
    "    df['ESC_PAI'].isin(['B', 'C', 'D']),               # Não tem EM completo\n",
    "    df['ESC_PAI'].isin(['E', 'F', 'G']),               # Tem EM completo\n",
    "    df['ESC_PAI'].isin(['H'])                         # Não sei\n",
    "]\n",
    "\n",
    "valores = [1, 2, 3, 4]\n",
    "\n",
    "df['ESC_PAI'] =  np.select(condicoes, valores)\n",
    "df['ESC_PAI'] = df['ESC_PAI'].astype('category')\n",
    "\n",
    "# ESC_MAE\n",
    "condicoes = [\n",
    "    df['ESC_MAE'].isin(['A']),                         # Nunca estudou\n",
    "    df['ESC_MAE'].isin(['B', 'C', 'D']),               # Não tem EM completo\n",
    "    df['ESC_MAE'].isin(['E', 'F', 'G']),               # Tem EM completo\n",
    "    df['ESC_MAE'].isin(['H'])                           # Não sei\n",
    "]\n",
    "\n",
    "valores = [1, 2, 3, 4]\n",
    "\n",
    "df['ESC_MAE'] =  np.select(condicoes, valores)\n",
    "df['ESC_MAE'] = df['ESC_MAE'].astype('category')\n",
    "\n",
    "# RENDA\n",
    "condicoes = [\n",
    "    df['Q006'] == 'A',                  # Sem renda                    \n",
    "    df['Q006'].isin(['B', 'C']),        # Renda até 1.5 SM\n",
    "    (~df['Q006'].isin(['A', 'B', 'C']))   # Renda maior que 1.5 SM\n",
    "]\n",
    "\n",
    "valores = [1, 2, 3]\n",
    "\n",
    "df['RENDA'] = np.select(condicoes, valores)\n",
    "df['RENDA'] = df['RENDA'].astype(int).astype('category')\n",
    "\n",
    "del df['Q006']\n",
    "\n",
    "# TP_STATUS_REDACAO\n",
    "condicoes = [\n",
    "    df['TP_STATUS_REDACAO'] == '1.0',\n",
    "    df['TP_STATUS_REDACAO'] == '2.0',\n",
    "    df['TP_STATUS_REDACAO'] == '3.0',\n",
    "    df['TP_STATUS_REDACAO'] == '4.0',\n",
    "    df['TP_STATUS_REDACAO'] == '6.0',\n",
    "    df['TP_STATUS_REDACAO'] == '7.0',\n",
    "    df['TP_STATUS_REDACAO'] == '8.0',\n",
    "    df['TP_STATUS_REDACAO'] == '9.0',\n",
    "    df['TP_STATUS_REDACAO'].isna(),\n",
    "]\n",
    "\n",
    "valores = [1, 2, 3, 4, 6, 7, 8, 9, 10]\n",
    "\n",
    "df['TP_STATUS_REDACAO'] =  np.select(condicoes, valores)\n",
    "df['TP_STATUS_REDACAO'] = df['TP_STATUS_REDACAO'].astype('category')\n",
    "\n",
    "# VARIÁVEIS NUMÉRICAS\n",
    "# Transformacao das variaveis numericas em categoricas, segunda as faixas estipuladas\n",
    "colunas = ['NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_LC', 'NU_NOTA_MT',  'NU_NOTA_REDACAO',\n",
    "           'NU_NOTA_COMP1', 'NU_NOTA_COMP2', 'NU_NOTA_COMP3', 'NU_NOTA_COMP4', 'NU_NOTA_COMP5']\n",
    "\n",
    "for col in colunas:\n",
    "    mediana = df[col].median()\n",
    "    std = df[col].std()\n",
    "\n",
    "    condicoes = [\n",
    "        df[col].isna(),\n",
    "        df[col] == 0,\n",
    "        df[col] <= mediana,\n",
    "        df[col] > mediana\n",
    "    ]\n",
    "\n",
    "    valores = [0, 1, 2, 3]\n",
    "\n",
    "    df[col] =  np.select(condicoes, valores)\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "# # Proporcao das faixas das notas\n",
    "# print(\"\\n:\")\n",
    "\n",
    "# colunas = ['NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_LC', 'NU_NOTA_MT', 'NU_NOTA_REDACAO',\n",
    "#            'NU_NOTA_COMP1', 'NU_NOTA_COMP2', 'NU_NOTA_COMP3', 'NU_NOTA_COMP4', 'NU_NOTA_COMP5']\n",
    "# contagens = {}\n",
    "# percentual = {}\n",
    "\n",
    "# for col in colunas:\n",
    "    \n",
    "#     contagens[f'tp_{col}'] = df[col].value_counts(dropna=False)\n",
    "#     percentual[f'tp_{col}_pct'] = df[col].value_counts(normalize=True, dropna=False) * 100\n",
    "\n",
    "#     resultado = pd.DataFrame({\n",
    "#         'Frequência': contagens[f'tp_{col}'],\n",
    "#         'Percentual (%)': percentual[f'tp_{col}_pct'].round(2)\n",
    "#     })\n",
    "    \n",
    "#     print(resultado, '\\n')\n",
    "\n",
    "# Exclusoes mais novas\n",
    "del df['TP_NACIONALIDADE']\n",
    "del df['TP_ESTADO_CIVIL']\n",
    "del df['CO_PROVA_DIA1']\n",
    "del df['CO_PROVA_DIA2']\n",
    "del df['TP_STATUS_REDACAO']\n",
    "\n",
    "# # Informações para conferência\n",
    "# print(\"=\"*40)\n",
    "# print(f\"Memória usada: {df.memory_usage(deep=True).sum() / (1024 ** 2):.2f} MB\")\n",
    "# print(f\"Quantidade de colunas: {df.shape[1]}\")\n",
    "# print(f\"Quantidade de linhas: {df.shape[0]}\")\n",
    "# print(\"=\"*40)\n",
    "\n",
    "# # Tipos de dados\n",
    "# print(\"\\nTipos de dados por coluna:\")\n",
    "# print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48dc53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='category').columns:\n",
    "    valores = df[col].unique().tolist()\n",
    "    try:\n",
    "        # Tenta converter para número e ordenar\n",
    "        valores_ordenados = sorted(valores, key=lambda x: float(x))\n",
    "    except ValueError:\n",
    "        # Se não forem numéricos, mantém a ordem original\n",
    "        valores_ordenados = valores\n",
    "    print(f'\\n{col}: {valores_ordenados}')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb684a40",
   "metadata": {},
   "source": [
    "## Checa significâncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce32d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_barras_categoricas\n",
    "\n",
    "desconsidera_cols = ['NU_NOTA_COMP1', 'NU_NOTA_COMP2', 'NU_NOTA_COMP3', 'NU_NOTA_COMP4', 'NU_NOTA_COMP5']\n",
    "plot_barras_categoricas(df.drop(columns=desconsidera_cols), min_sup=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534405cd",
   "metadata": {},
   "source": [
    "## Aplicando o algoritmo (suporte = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513563f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma dataframe em formato transacional\n",
    "\n",
    "desconsidera_cols = ['NU_NOTA_COMP1', 'NU_NOTA_COMP2', 'NU_NOTA_COMP3', 'NU_NOTA_COMP4', 'NU_NOTA_COMP5']\n",
    "df_binario = pd.get_dummies(df.drop(columns=desconsidera_cols), prefix_sep='_', dtype=bool)\n",
    "\n",
    "print(f'Total de colunas: {len(df_binario.columns)}')\n",
    "for col in df_binario.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b83d988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, fpgrowth\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "import time\n",
    "\n",
    "# 1. Gerar itemsets frequentes usando FP-Growth\n",
    "print('Gerando itemsets...')\n",
    "\n",
    "start_time = time.time()\n",
    "frequent_itemsets = fpgrowth(df_binario, min_support=0.33, use_colnames=True)\n",
    "end_time = time.time()\n",
    "print(f\"\\tTempo de execução: {end_time - start_time} segundos\")\n",
    "\n",
    "# 2. Visualizar os itemsets frequentes\n",
    "print(frequent_itemsets)\n",
    "\n",
    "# 3. Gerar regras de associação a partir dos itemsets\n",
    "start_time = time.time()\n",
    "regras = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
    "end_time = time.time()\n",
    "print(f\"\\tTempo de execução: {end_time - start_time} segundos\")\n",
    "\n",
    "# 4. Visualizar as regras\n",
    "print(regras[['antecedents', 'consequents', 'support', 'confidence', 'lift']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f38ab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporta pra csv\n",
    "RESULTS_PATH = Path().resolve() / 'results'\n",
    "frequent_itemsets.to_csv(RESULTS_PATH / 'freq_itemsets_new_33.csv')\n",
    "regras.to_csv(RESULTS_PATH / 'regras_new_33_conf50.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b412ff6",
   "metadata": {},
   "source": [
    "## Analisando apenas quem foi os dois dias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c05bebd",
   "metadata": {},
   "source": [
    "Para focar a análise e encontrar padrões relevantes, decidimos focar a análise apenas nos candidatos que foram nos dois dias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f966b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_barras_categoricas\n",
    "\n",
    "df_apenas_presentes = df[(df.TP_PRESENCA_DIA1 == '1') & (df.TP_PRESENCA_DIA2 == '1')].copy()\n",
    "df_apenas_presentes.drop(columns=['TP_PRESENCA_DIA2', 'TP_PRESENCA_DIA1'], inplace=True)\n",
    "print(f'Total anterior:\\t {len(df)}')\n",
    "print(f'Total novo:\\t {len(df_apenas_presentes)}')\n",
    "\n",
    "plot_barras_categoricas(df_apenas_presentes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00268a4c",
   "metadata": {},
   "source": [
    "## Analisando apenas quem faltou pelo menos um dia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c25a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_barras_categoricas\n",
    "\n",
    "df_apenas_presentes = df[(df.TP_PRESENCA_DIA1 != '1') | (df.TP_PRESENCA_DIA2 != '1')].copy()\n",
    "# df_apenas_presentes.drop(columns=['TP_PRESENCA_DIA2', 'TP_PRESENCA_DIA1'], inplace=True)\n",
    "print(f'Total anterior:\\t {len(df)}')\n",
    "print(f'Total novo:\\t {len(df_apenas_presentes)}')\n",
    "\n",
    "plot_barras_categoricas(df_apenas_presentes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a0c5ab",
   "metadata": {},
   "source": [
    "## Apenas não treineiros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef5808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_barras_categoricas\n",
    "\n",
    "df_apenas_nao_treineiros = df[(df.IN_TREINEIRO == '0')].copy()\n",
    "df_apenas_nao_treineiros.drop(columns=['IN_TREINEIRO'], inplace=True)\n",
    "print(f'Total anterior:\\t {len(df)}')\n",
    "print(f'Total novo:\\t {len(df_apenas_nao_treineiros)}')\n",
    "\n",
    "plot_barras_categoricas(df_apenas_nao_treineiros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b9a445",
   "metadata": {},
   "source": [
    "## Apenas treineiros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad311374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_barras_categoricas\n",
    "\n",
    "df_apenas_treineiros = df[(df.IN_TREINEIRO == '1')].copy()\n",
    "df_apenas_treineiros.drop(columns=['IN_TREINEIRO'], inplace=True)\n",
    "print(f'Total anterior:\\t {len(df)}')\n",
    "print(f'Total novo:\\t {len(df_apenas_treineiros)}')\n",
    "\n",
    "plot_barras_categoricas(df_apenas_treineiros)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
